{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":214002036,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n \n# To delete all files in the /kaggle/working/ directory\nfolder_path = '/kaggle/working'\n \n# Check if the folder exists\nif os.path.exists(folder_path):\n    # Delete all files in the folder\n    for filename in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n    print(f'All files in {folder_path} have been deleted.')\nelse:\n    print(f'{folder_path} does not exist.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the source and destination directories\nsource_dir = '/kaggle/input/chatbot-new/'\ndestination_dir = '/kaggle/working/'\n\n# Copy all files from source to destination, maintaining the directory structure\nfor dirname, _, filenames in os.walk(source_dir):\n    for filename in filenames:\n        # Construct the full file paths\n        source_file = os.path.join(dirname, filename)\n        # Create the corresponding destination directory\n        relative_path = os.path.relpath(dirname, source_dir)\n        destination_subdir = os.path.join(destination_dir, relative_path)\n        os.makedirs(destination_subdir, exist_ok=True)\n        destination_file = os.path.join(destination_subdir, filename)\n        # Copy the file\n        shutil.copy(source_file, destination_file)\n\nprint(\"All files have been copied to /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:03:04.436311Z","iopub.execute_input":"2024-12-23T05:03:04.437023Z","iopub.status.idle":"2024-12-23T05:04:19.455644Z","shell.execute_reply.started":"2024-12-23T05:03:04.436996Z","shell.execute_reply":"2024-12-23T05:04:19.454632Z"}},"outputs":[{"name":"stdout","text":"All files have been copied to /kaggle/working/\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install datasets transformers scikit-learn torch accelerate rouge-score nltk evaluate tensorboard tensorboardX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:04:26.505607Z","iopub.execute_input":"2024-12-23T05:04:26.505948Z","iopub.status.idle":"2024-12-23T05:04:40.866037Z","shell.execute_reply.started":"2024-12-23T05:04:26.505919Z","shell.execute_reply":"2024-12-23T05:04:40.865106Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.6)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (70.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e4db559f2825fb207971cb65df982b2b6885d0f03cb067715097acdb4b886e7d\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score, evaluate\nSuccessfully installed evaluate-0.4.3 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\nimport evaluate\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, default_data_collator, EarlyStoppingCallback, TrainerCallback, AutoModelForCausalLM\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport torch\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:04:40.868148Z","iopub.execute_input":"2024-12-23T05:04:40.868855Z","iopub.status.idle":"2024-12-23T05:04:54.895896Z","shell.execute_reply.started":"2024-12-23T05:04:40.868811Z","shell.execute_reply":"2024-12-23T05:04:54.894763Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model_name = \"facebook/bart-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:04:54.897387Z","iopub.execute_input":"2024-12-23T05:04:54.898273Z","iopub.status.idle":"2024-12-23T05:04:59.499200Z","shell.execute_reply.started":"2024-12-23T05:04:54.898227Z","shell.execute_reply":"2024-12-23T05:04:59.498500Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a607884715614c1a808d5d1ef5f5bfec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15e5cb102ad744e2bbd074d15aa4f88b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25d87d5ba6a4a028e18d8eb83e9ea1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a372f9ffd14d28a7fee5752e2d4e76"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3b17361e894661aa9f8b444d0865e6"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:05:13.050674Z","iopub.execute_input":"2024-12-23T05:05:13.051015Z","iopub.status.idle":"2024-12-23T05:05:14.277138Z","shell.execute_reply.started":"2024-12-23T05:05:13.050985Z","shell.execute_reply":"2024-12-23T05:05:14.276076Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define preprocessing functions for dataset1\ndef extract_text(data):\n    parts = data.split(\"Text:\")\n    extracted_texts = []\n    for part in parts[1:]:\n        text = part.split(\"Document:\")[0].strip()\n        extracted_texts.append(text)\n    updated_data = \"\\n\".join(extracted_texts)\n    return updated_data\n \ndef process_text(data):\n    if 'Cited Documents: None' in data:\n        return None\n    answer_start = data.find('Answer:') + len('Answer:')\n    answer_text = data[answer_start:].strip()\n    cleaned_text = re.sub(r'<[^>]*>', '', answer_text)\n    return cleaned_text\n \ndef preprocess_function_dataset1(examples):\n    questions = examples[\"question\"]\n    documents = examples[\"documents\"]\n    targets = examples[\"answer\"]\n \n    # Apply text extraction and processing\n    texts = [extract_text(d) for d in documents]\n    processed_targets = [process_text(t) for t in targets]\n \n    # Filter out examples with invalid targets\n    valid_indices = [i for i, target in enumerate(processed_targets) if target is not None]\n    questions = [questions[i] for i in valid_indices]\n    texts = [texts[i] for i in valid_indices]\n    targets = [processed_targets[i] for i in valid_indices]\n \n    # Filter out documents with more than 1000 words\n    valid_indices = [i for i, text in enumerate(texts) if len(text.split()) <= 1000]\n    questions = [questions[i] for i in valid_indices]\n    texts = [texts[i] for i in valid_indices]\n    targets = [targets[i] for i in valid_indices]\n \n    # Tokenize questions and texts separately (without padding yet)\n    # question_encodings = tokenizer(questions, padding=False, truncation=False)\n    # text_encodings = tokenizer(texts, padding=False, truncation=False)\n \n    # Filter out examples exceeding max length *before* padding or truncation\n    inputs = []\n    final_targets = []\n\n    for q, t, target in zip(questions, texts, targets):\n        input_text = f\"Answer the following question based on the provided text:\\n\\nQuestion: {q} \\n\\nText: {t}\"\n        encodings = tokenizer(input_text, padding=False, truncation=False)\n\n        if len(encodings.input_ids) <= 1024:\n            inputs.append(encodings)\n            final_targets.append(target)\n \n    # Now pad the filtered examples\n    model_inputs = tokenizer.pad(inputs, padding=\"max_length\", max_length=1024, return_tensors=\"pt\")\n \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(final_targets, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Define preprocessing function for dataset2\ndef preprocess_function_dataset2(examples):\n    questions = examples[\"question\"]\n    documents = examples[\"documents\"]\n    targets = examples[\"answer\"]\n \n    # Ensure documents, questions, and targets are strings\n    documents = [str(doc) for doc in documents]\n    questions = [str(q) for q in questions]\n    targets = [str(target) for target in targets]\n \n    # Filter out documents with more than 1000 words\n    valid_indices = [i for i, doc in enumerate(documents) if len(doc.split()) <= 1000]\n    questions = [questions[i] for i in valid_indices]\n    documents = [documents[i] for i in valid_indices]\n    targets = [targets[i] for i in valid_indices]\n \n    # Tokenize questions and documents separately (without padding yet)\n    # question_encodings = tokenizer(questions, padding=False, truncation=False)\n    # document_encodings = tokenizer(documents, padding=False, truncation=False)\n \n    # Filter out examples exceeding max length *before* padding or truncation\n    inputs = []\n    final_targets = []\n\n    for q, d, target in zip(questions, documents, targets):\n        input_text = f\"Answer the following question based on the provided text:\\n\\nQuestion: {q} \\n\\nText: {d}\"\n        encodings = tokenizer(input_text, padding=False, truncation=False)\n\n        if len(encodings.input_ids) <= 1024:\n            inputs.append(encodings)\n            final_targets.append(target)\n \n    # Now pad the filtered examples\n    model_inputs = tokenizer.pad(inputs, padding=\"max_length\", max_length=1024, return_tensors=\"pt\")\n \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(final_targets, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:40:32.050345Z","iopub.execute_input":"2024-12-18T09:40:32.051059Z","iopub.status.idle":"2024-12-18T09:40:32.064807Z","shell.execute_reply.started":"2024-12-18T09:40:32.051027Z","shell.execute_reply":"2024-12-18T09:40:32.063965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and Prepare Dataset 1\ndataset1 = load_dataset(\"glaiveai/RAG-v1\")\n \n# Filter out examples with answer_mode = \"Mixed\" in dataset1\ndataset1 = dataset1.filter(lambda example: example['answer_mode'] == 'Grounded')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:40:36.802640Z","iopub.execute_input":"2024-12-18T09:40:36.803477Z","iopub.status.idle":"2024-12-18T09:40:48.658860Z","shell.execute_reply.started":"2024-12-18T09:40:36.803440Z","shell.execute_reply":"2024-12-18T09:40:48.658081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and Prepare Dataset 1\ndataset2 = load_dataset(\"neural-bridge/rag-dataset-12000\")\n\n# Rename columns in dataset2 to match dataset1\ndataset2 = dataset2.rename_column(\"context\", \"documents\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:40:48.660543Z","iopub.execute_input":"2024-12-18T09:40:48.661143Z","iopub.status.idle":"2024-12-18T09:40:51.815920Z","shell.execute_reply.started":"2024-12-18T09:40:48.661102Z","shell.execute_reply":"2024-12-18T09:40:51.815305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess dataset1\ndataset1 = dataset1.map(preprocess_function_dataset1, batched=True, remove_columns=['documents', 'answer', 'question', 'system_prompt', 'answer_mode'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess dataset2\ndataset2 = dataset2.map(preprocess_function_dataset2, batched=True, remove_columns=['documents', 'question', 'answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:41:56.354771Z","iopub.execute_input":"2024-12-18T09:41:56.355052Z","iopub.status.idle":"2024-12-18T09:42:24.949307Z","shell.execute_reply.started":"2024-12-18T09:41:56.355026Z","shell.execute_reply":"2024-12-18T09:42:24.948544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine the datasets\ncombined_dataset = concatenate_datasets([dataset1[\"train\"], dataset2[\"train\"], dataset2[\"test\"]])\n \n# Shuffle the combined dataset\ncombined_dataset = combined_dataset.shuffle(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:42:24.950316Z","iopub.execute_input":"2024-12-18T09:42:24.950591Z","iopub.status.idle":"2024-12-18T09:42:24.971102Z","shell.execute_reply.started":"2024-12-18T09:42:24.950565Z","shell.execute_reply":"2024-12-18T09:42:24.970489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the combined dataset\ntrain_dataset, temp_dataset = combined_dataset.train_test_split(test_size=0.2, seed=42).values()\nval_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42).values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:42:29.660809Z","iopub.execute_input":"2024-12-18T09:42:29.661160Z","iopub.status.idle":"2024-12-18T09:42:29.689703Z","shell.execute_reply.started":"2024-12-18T09:42:29.661130Z","shell.execute_reply":"2024-12-18T09:42:29.689097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of examples in each split\nprint(f\"Number of examples in train dataset: {len(train_dataset)}\")\nprint(f\"Number of examples in validation dataset: {len(val_dataset)}\")\nprint(f\"Number of examples in test dataset: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:42:32.949190Z","iopub.execute_input":"2024-12-18T09:42:32.949811Z","iopub.status.idle":"2024-12-18T09:42:32.954382Z","shell.execute_reply.started":"2024-12-18T09:42:32.949776Z","shell.execute_reply":"2024-12-18T09:42:32.953512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the splits to your directory\ntrain_dataset.save_to_disk(\"train_dataset\")\nval_dataset.save_to_disk(\"val_dataset\")\ntest_dataset.save_to_disk(\"test_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:43:39.159883Z","iopub.execute_input":"2024-12-18T09:43:39.160250Z","iopub.status.idle":"2024-12-18T09:43:39.752162Z","shell.execute_reply.started":"2024-12-18T09:43:39.160220Z","shell.execute_reply":"2024-12-18T09:43:39.751277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the splits from your directory\ntrain_dataset = load_from_disk(\"train_dataset\")\nval_dataset = load_from_disk(\"val_dataset\")\ntest_dataset = load_from_disk(\"test_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:05:29.469013Z","iopub.execute_input":"2024-12-23T05:05:29.469876Z","iopub.status.idle":"2024-12-23T05:05:29.485994Z","shell.execute_reply.started":"2024-12-23T05:05:29.469844Z","shell.execute_reply":"2024-12-23T05:05:29.485277Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:05:32.413663Z","iopub.execute_input":"2024-12-23T05:05:32.414035Z","iopub.status.idle":"2024-12-23T05:05:33.520579Z","shell.execute_reply.started":"2024-12-23T05:05:32.414003Z","shell.execute_reply":"2024-12-23T05:05:33.519585Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n      (layers): ModuleList(\n        (0-5): 6 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=100,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n#     eval_steps=1000,\n#     save_steps=1000,\n    save_total_limit=4,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"all\",\n    fp16=True,\n    max_grad_norm=1.0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:05:31.364280Z","iopub.execute_input":"2024-12-20T06:05:31.364561Z","iopub.status.idle":"2024-12-20T06:05:31.394211Z","shell.execute_reply.started":"2024-12-20T06:05:31.364534Z","shell.execute_reply":"2024-12-20T06:05:31.393602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\noptimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n    optimizers=(optimizer, None)\n)\n\ncheckpoint_path = \"/kaggle/working/results/checkpoint-11193\"\ntrainer.train(resume_from_checkpoint=checkpoint_path)\n# trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T06:06:18.764869Z","iopub.execute_input":"2024-12-20T06:06:18.765787Z","iopub.status.idle":"2024-12-20T10:41:31.512025Z","shell.execute_reply.started":"2024-12-20T06:06:18.765740Z","shell.execute_reply":"2024-12-20T10:41:31.511298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(\"./best_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T10:41:31.513758Z","iopub.execute_input":"2024-12-20T10:41:31.514775Z","iopub.status.idle":"2024-12-20T10:41:32.824158Z","shell.execute_reply.started":"2024-12-20T10:41:31.514733Z","shell.execute_reply":"2024-12-20T10:41:32.823474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_test_example = test_dataset[0]\n\n# Decode and print the question and text\ninput_ids = first_test_example[\"input_ids\"]\ndecoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\nprint(\"Input Question and Text:\")\nprint(decoded_input)\nprint(\"-\" * 20) # Separator\n\n# Prepare input_ids for the model\ninput_ids = torch.tensor([input_ids]).to(device)\n\n# 3. Generate and Decode the Output\nwith torch.no_grad():\n    generated_ids = model.generate(input_ids=input_ids, max_length=256)\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n# 4. Print the Generated and Reference Texts\nprint(\"Generated Answer:\")\nprint(generated_text)\nreference_text = tokenizer.decode(first_test_example[\"labels\"], skip_special_tokens=True)\nprint(\"\\nReference Answer:\")\nprint(reference_text)\n\n# 5. (Optional) Calculate Loss\nlabels = torch.tensor([first_test_example[\"labels\"]]).to(device)\noutputs = model(**{\"input_ids\": input_ids}, labels=labels) # Pass input_ids as a dictionary\nloss = outputs.loss\nprint(f\"Calculated Loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T09:39:11.900057Z","iopub.execute_input":"2024-11-19T09:39:11.900419Z","iopub.status.idle":"2024-11-19T09:39:12.407578Z","shell.execute_reply.started":"2024-11-19T09:39:11.900389Z","shell.execute_reply":"2024-11-19T09:39:12.406573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_question = \"What treaties are being talked about related to the panama canal dispute?\"\ncustom_text = \"\"\"US President elect Donald Trump has threatened to demand the return of the Panama Canal to the United States if Panama continues to charge what he described as \"ridiculous\" passage fees for American vessels. In a post on his social media platform Truth social, Trump remarked that \"the Panama Canal is considered a VITAL National Asset for the United States, due to its critical role to America's Economy and National Security\", and strongly criticised the tolls levied, which can from as low as $0.50 to as high as $300,000. \"The fees being charged by Panama are ridiculous, especially knowing the extraordinary generosity that has been bestowed to Panama by the US. This complete 'rip-off' of our country will immediately stop,\" Trump wrote. The US completed the construction of the canal in 1914 and managed it until December 31, 1999. On that date, control of the canal was officially handed over to Panama, a sovereign country, based on treaties signed in 1997.  The US is the largest user of the Canal. \"It was not given for the benefit of others, but merely as a token of cooperation with us and Panama. If the principles, both moral and legal, of this magnanimous gesture of giving are not followed, then we will demand that the Panama Canal be returned to us, in full, and without question,\" Trump added. \"\"\"\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# 1. Load Your Fine-tuned Model and Tokenizer\nmodel_path = \"./best_model\"  # Path to your saved model directory\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# Combine question and text into a single input string\ninput_text = f\"Answer the following question based on the provided text:\\n\\nQuestion: {custom_question} \\n\\nText: {custom_text}\"\n\n# 3. Tokenize the Input\ninputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n\n# 4. Generate and Decode the Answer\nwith torch.no_grad():\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=256,\n        num_beams=4,\n    )\ngenerated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n# 5. Print the Generated Answer\nprint(\"Generated Answer:\")\nprint(generated_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:07:15.445078Z","iopub.execute_input":"2024-12-23T05:07:15.445924Z","iopub.status.idle":"2024-12-23T05:07:17.293917Z","shell.execute_reply.started":"2024-12-23T05:07:15.445890Z","shell.execute_reply":"2024-12-23T05:07:17.293042Z"}},"outputs":[{"name":"stdout","text":"Generated Answer:\nThe treaties are being talked about related to the panama canal dispute. The US is the largest user of the Canal. It was not given for the benefit of others, but merely as a token of cooperation with us and Panama. If the principles, both moral and legal, of this magnanimous gesture of giving are not followed, then we will demand that the Panama Canal be returned to us, in full, and without question.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for i in range(3):\n    print(f\"--- Training Example {i+1} ---\")\n    # print(f\"Input IDs: {train_dataset[i]['input_ids']}\")\n    print(f\"Decoded Input: {tokenizer.decode(train_dataset[i]['input_ids'])}\")\n    # print(f\"Label IDs: {train_dataset[i]['labels']}\")\n    print(f\"Decoded Label: {tokenizer.decode(train_dataset[i]['labels'])}\")\n\n    print(f\"--- Validation Example {i+1} ---\")\n    # print(f\"Input IDs: {val_dataset[i]['input_ids']}\")\n    print(f\"Decoded Input: {tokenizer.decode(val_dataset[i]['input_ids'])}\")\n    # print(f\"Label IDs: {val_dataset[i]['labels']}\")\n    print(f\"Decoded Label: {tokenizer.decode(val_dataset[i]['labels'])}\")\n\n    print(f\"--- Test Example {i+1} ---\")\n    # print(f\"Input IDs: {test_dataset[i]['input_ids']}\")\n    print(f\"Decoded Input: {tokenizer.decode(test_dataset[i]['input_ids'])}\")\n    # print(f\"Label IDs: {test_dataset[i]['labels']}\")\n    print(f\"Decoded Label: {tokenizer.decode(test_dataset[i]['labels'])}\")\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:42:55.207077Z","iopub.execute_input":"2024-12-18T09:42:55.207433Z","iopub.status.idle":"2024-12-18T09:42:55.326827Z","shell.execute_reply.started":"2024-12-18T09:42:55.207406Z","shell.execute_reply":"2024-12-18T09:42:55.325997Z"}},"outputs":[],"execution_count":null}]}